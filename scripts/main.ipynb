{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from functions import *\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set device in order to determine which device are we using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor    = 4\n",
    "batch_size      = 32\n",
    "epoch           = 1\n",
    "learning_rate   = 0.0003\n",
    "criterion       = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set our hyper-parameter here. We have scale factor = 4 which means we are transforming 128 x 128 picture to 512 x 512 picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/train/wild'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/princess/Documents/vignette-super-resolution/scripts/main.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/princess/Documents/vignette-super-resolution/scripts/main.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_data \u001b[39m=\u001b[39m DatasetFromFolder(\u001b[39m\"\u001b[39;49m\u001b[39m../../../data/train/wild\u001b[39;49m\u001b[39m\"\u001b[39;49m, scale_factor\u001b[39m=\u001b[39;49mscale_factor)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/princess/Documents/vignette-super-resolution/scripts/main.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_data \u001b[39m=\u001b[39m DatasetFromFolder(\u001b[39m\"\u001b[39m\u001b[39m../../../data/train/wild\u001b[39m\u001b[39m\"\u001b[39m, scale_factor\u001b[39m=\u001b[39mscale_factor)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/princess/Documents/vignette-super-resolution/scripts/main.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ref_data \u001b[39m=\u001b[39m DatasetFromFolder(\u001b[39m\"\u001b[39m\u001b[39m../../../data/loss\u001b[39m\u001b[39m\"\u001b[39m, scale_factor\u001b[39m=\u001b[39mscale_factor)\n",
      "File \u001b[0;32m~/Documents/vignette-super-resolution/scripts/functions.py:33\u001b[0m, in \u001b[0;36mDatasetFromFolder.__init__\u001b[0;34m(self, image_dir, scale_factor)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, image_dir, scale_factor\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[39msuper\u001b[39m(DatasetFromFolder, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_filenames \u001b[39m=\u001b[39m [join(image_dir, x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m listdir(image_dir) \u001b[39mif\u001b[39;00m is_image_file(x)]\n\u001b[1;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/train/wild'"
     ]
    }
   ],
   "source": [
    "train_data = DatasetFromFolder(\"../../../data/train/wild\", scale_factor=scale_factor)\n",
    "test_data = DatasetFromFolder(\"../../../data/train/wild\", scale_factor=scale_factor)\n",
    "ref_data = DatasetFromFolder(\"../../../data/loss\", scale_factor=scale_factor)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "ref_dataloader = DataLoader(ref_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our data loader here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(scale_factor=scale_factor).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(3, epoch)\n",
    "figure.set_size_inches(20, 15)\n",
    "\n",
    "loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    epoch_loss_history, epoch_test_loss_history = train(i, model)\n",
    "    loss_history = loss_history + epoch_loss_history\n",
    "    test_loss_history = test_loss_history + epoch_test_loss_history\n",
    "    del(epoch_loss_history)\n",
    "    del(epoch_test_loss_history)\n",
    "    \n",
    "    ref = next(iter(ref_dataloader))[0]\n",
    "    ref_tgt = next(iter(ref_dataloader))[1]\n",
    "    ref_fit = model(ref.to(device)).cpu()\n",
    "    \n",
    "    ref = swap(ref.squeeze())\n",
    "    ref_tgt = swap(ref_tgt.squeeze())\n",
    "    ref_fit = swap(ref_fit.detach().numpy().squeeze())\n",
    "\n",
    "    ax[0, i].imshow(ref)\n",
    "    ax[1, i].imshow(ref_fit)\n",
    "    ax[2, i].imshow(ref_tgt)\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.plot(loss_history)\n",
    "plt.plot(test_loss_history)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
