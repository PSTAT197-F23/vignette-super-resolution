{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0391f7ed",
   "metadata": {},
   "source": [
    "# PSTAT 197A Final Project Report - ESPCNN (ISR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b367e",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08383e8",
   "metadata": {},
   "source": [
    "Our goal is to construct a model which that can reconstruct and de-blur images. The ESPCNN model, a type of ISR model, designed for image restoration, aiming to recover a high-resolution (HR) image from its corresponding low-resolution (LR) counterpart. Our image dataset consists of around 4,000 low-resolution images of wild animals from Kaggle(https://www.kaggle.com/datasets/dimensi0n/afhq-512?select=wild), each with dimensions of 3x512x512 (3 RGB color channel represents colored images instead of grey-scaled images, with 512x512 pixel length and width). To simulate the process of recovering high-resolution images from low-resolution inputs, we downscaled the high-resolution images to create new low-resolution images. Then, we set up the ESPCNN model with appropriate hyperparameters. Lastly, training the model on our prepared dataset. This involves feeding in the low-resolution images and training the model to output high-resolution images that match the upscaled versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f8fc1",
   "metadata": {},
   "source": [
    "## Dataset and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408227c",
   "metadata": {},
   "source": [
    "To prepare the images for input into our model, we need to load the dataset into pairs of input and target that the model can read. First we make a copy of the image, and downsample one image by the scale factor. The scale factor is a hyperparameter and we choose 4, thus the resulting input image has the size 128x128. Notice that the convolution and pixel shuffle layers do not restrict the input image shape, so images of any size can be feed into the model besides the square images we use. Then, we convert the images into tensors and put them into the Pytorch dataloader object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b648436",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4265b0",
   "metadata": {},
   "source": [
    "![alt text](https://raw.githubusercontent.com/PSTAT197-F23/vignette-super-resolution/main/image/model_explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6476abe",
   "metadata": {},
   "source": [
    "The model we use is called an efficient sub-pixel convolutional neural network(ESPCN). As we can see on the diagram, we first feed the model with a low resolution image, and we apply CNN layer to get feature maps. With those feature maps we can get number of channels that is greater than the original 3. With these channels we applied the pixel shuffle method which is the most important step for the sub-pixel convolution network. The concept of sub-pixel is that we believe that we have tiny pixels between the two physical pixels in the microscopic world. The reason for this sub-pixel convolution network is to reveal this sub-pixel relationship. As we can see on the diagram, each pixel on the feature maps represents each sub-pixel on the high resolution image. Our model contains three convolution layers which increase the channel size to 48, $3 \\times \\text{scale factor}^2$. With these channels we can use the SPC layer in order to get the high resolution picture. In addition, the activation function we used was ReLU, because it can turn the linear relationships into non-linear ones, and it is not computationally heavy which means we can train more epochs with less time and get better results. The loss function we used was MSE. The original 512 x 512 image would be the label and the down-sampled 128 x 128 picture was our input. The loss function is shown below:<br> \n",
    "$\\ell(W_{1:L, b_{1:L}}) = \\frac{1}{r^2HW}\\sum\\limits_{x=1}^{rH}\\sum\\limits_{x=1}^{rW}\\left( I^{HR}_{x,y} - f^L_{x,y}(I^{LR}) \\right)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb8f32",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b1303",
   "metadata": {},
   "source": [
    "The efficacy of our model is assessed through a comparative analysis of the input low-resolution images and the reconstructed high-resolution outputs, alongside the monitoring of the model's learning progress through a loss graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5185f",
   "metadata": {},
   "source": [
    "![alt text](https://raw.githubusercontent.com/PSTAT197-F23/vignette-super-resolution/main/image/reference.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a43bf9",
   "metadata": {},
   "source": [
    "The first image above provides a visual representation of the model's performance. The top row is the original image that was input into the model for training. As we can see, they are downsized. The second row shows work of our model. The enhanced clarity and detail in the reconstructed images are evident when compared to their low-resolution counterparts, showcasing the model's capability to recover fine details and sharpness that were not discernible in the original images. The bottom row is their corresponding high-resolution outputs at a size of 512x512 pixels for comparison. Thus, it's easy to tell that our model performed quite good in recovring low-resolution images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e291c",
   "metadata": {},
   "source": [
    "![alt text](https://raw.githubusercontent.com/PSTAT197-F23/vignette-super-resolution/main/image/loss.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73cc11",
   "metadata": {},
   "source": [
    "The second image depicts training and testing loss curves, which are essential indicator of the model's learning process over time. The blue line represents the training loss while the orange line indicates the test loss. The training loss is the measure of how well the model is fitting the training data. It is expected to decrease over time as the model learns from the training data. The training loss is captured in the variable loss_history, which is populated within the train function each time the loss is calculated for a batch during training. The test loss measures how well the model generalizes to new, unseen data. The test loss is recorded in the variable test_loss_history, which is updated every 5 iterations within the training loop when the tloss is calculated using the test_dataloader. A steady decline in two loss values as training progresses is a positive sign, indicating that our model is effectively learning the task of super-resolution, and has no signs of overfitting at the current stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753d07e",
   "metadata": {},
   "source": [
    "These results are promising, suggesting that our model is capable of significantly improving quality of low-resolution images. The training and test loss trends further affirm that the model is learning as expected and is on the right path to achieving image reconstruction.\n",
    "\n",
    "However, besides MSE, we still need another interpretable metric to quantify the distance between the generated images and the original images and evaluate the model performance. In addition, no overfitting suggests that increasing the model size may potentially yield better results, which we haven't explore due to hardware constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e09963",
   "metadata": {},
   "source": [
    "## Code Appendix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21aff7",
   "metadata": {},
   "source": [
    "### Run the scripts to reproduce the result. Do not run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b9333",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    return img\n",
    "\n",
    "class DatasetFromFolder(Dataset):\n",
    "    def __init__(self, image_dir, scale_factor):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n",
    "        self.tensor = transforms.ToTensor()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = load_img(self.image_filenames[index])\n",
    "        target = input.copy()\n",
    "    \n",
    "        input = self.tensor(input)\n",
    "        target = self.tensor(target)\n",
    "        \n",
    "        height, width = transforms.functional.get_image_size(input)\n",
    "        resize = transforms.Resize((int(height/self.scale_factor), int(width/self.scale_factor)), \n",
    "                                  transforms.InterpolationMode.BICUBIC, \n",
    "                                  antialias=True\n",
    "                                 )\n",
    "        input = resize(input)\n",
    "        del(resize)\n",
    "        \n",
    "        return input, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4f465",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5082fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 3 * scale_factor ** 2, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.clamp(x, 0.0, 1.0)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07c1bf",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d88c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# import dataset.py and model.py\n",
    "from dataset import *\n",
    "from model import *\n",
    "\n",
    "\n",
    "# main helper functions\n",
    "def swap(img):\n",
    "    img = img.swapaxes(0, 1)\n",
    "    img = img.swapaxes(1, 2)\n",
    "    return img\n",
    "\n",
    "def train(epoch, model):\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_history = []\n",
    "    epoch_test_loss_history = []\n",
    "    \n",
    "    for iteration, batch in enumerate(train_dataloader, 1):\n",
    "        img, target = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(img), target)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % 5 == 0:\n",
    "            tbatch = next(iter(test_dataloader))\n",
    "            timg, ttarget = tbatch[0].to(device), tbatch[1].to(device)\n",
    "            tloss = criterion(model(timg), ttarget)\n",
    "            \n",
    "            epoch_loss_history.append(loss.item())\n",
    "            epoch_test_loss_history.append(tloss.item())\n",
    "            \n",
    "            print(\"===> Epoch[{}]({}/{}): Loss: {:.6f}, Test Loss: {:.6f}\".format(\n",
    "                epoch+1, iteration, len(train_dataloader), loss.item(), tloss.item()))\n",
    "\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.6f}\".format(epoch+1, epoch_loss / len(train_dataloader)))\n",
    "    \n",
    "    return epoch_loss_history, epoch_test_loss_history\n",
    "\n",
    "\n",
    "# set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    \n",
    "# set hyperparameter\n",
    "scale_factor    = 4\n",
    "batch_size      = 32\n",
    "epoch           = 5\n",
    "learning_rate   = 0.0003\n",
    "criterion       = nn.MSELoss()\n",
    "\n",
    "\n",
    "# load data\n",
    "train_data = DatasetFromFolder(\"../data/train/wild\", scale_factor=scale_factor)\n",
    "test_data = DatasetFromFolder(\"../data/train/wild\", scale_factor=scale_factor)\n",
    "ref_data = DatasetFromFolder(\"../data/loss\", scale_factor=scale_factor)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "ref_dataloader = DataLoader(ref_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# create model\n",
    "model = Model(scale_factor=scale_factor).to(device)\n",
    "\n",
    "\n",
    "# train\n",
    "figure, ax = plt.subplots(3, epoch)\n",
    "figure.set_size_inches(20, 15)\n",
    "\n",
    "loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    epoch_loss_history, epoch_test_loss_history = train(i, model)\n",
    "    loss_history = loss_history + epoch_loss_history\n",
    "    test_loss_history = test_loss_history + epoch_test_loss_history\n",
    "    del(epoch_loss_history)\n",
    "    del(epoch_test_loss_history)\n",
    "    \n",
    "    ref = next(iter(ref_dataloader))[0]\n",
    "    ref_tgt = next(iter(ref_dataloader))[1]\n",
    "    ref_fit = model(ref.to(device)).cpu()\n",
    "    \n",
    "    ref = swap(ref.squeeze())\n",
    "    ref_tgt = swap(ref_tgt.squeeze())\n",
    "    ref_fit = swap(ref_fit.detach().numpy().squeeze())\n",
    "\n",
    "    ax[0, i].imshow(ref)\n",
    "    ax[1, i].imshow(ref_fit)\n",
    "    ax[2, i].imshow(ref_tgt)\n",
    "\n",
    "figure.savefig('../image/reference.jpg')\n",
    "\n",
    "\n",
    "# plot loss\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(loss_history)\n",
    "plt.plot(test_loss_history)\n",
    "plt.savefig('../image/loss.jpg')\n",
    "\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), '../model/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
