{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0391f7ed",
   "metadata": {},
   "source": [
    "# PSTAT 197A Final Project Report - ESPCNN (ISR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b367e",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to develop a model specialized in image reconstruction and de-blurring. Specifically, we employ the ESPCN model, a form of Image Super-Resolution (ISR) model designed for image restoration. This model is tailored to recover high-resolution (HR) images from their corresponding low-resolution (LR) counterparts. Our dataset comprises approximately 4,000 low-resolution images of wild animals, sourced from Kaggle (https://www.kaggle.com/datasets/dimensi0n/afhq-512?select=wild), each possessing dimensions of 3x512x512 (with three RGB color channels representing colored images and 512 pixel length x512 pixel width)). To simulate the process of recuperating high-resolution images from low-resolution inputs, we systematically downscaled the high-resolution images, thereby generating new low-resolution counterparts. Subsequently, we configured the ESPCN model with appropriately chosen hyperparameters and initiated its training phase using our prepared dataset. This entails inputting the low-resolution images into the model and iteratively refining its parameters to yield high-resolution outputs congruent with the upscaled versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f8fc1",
   "metadata": {},
   "source": [
    "## Dataset and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the images for input into our model, the dataset is formatted into pairs of input and target images, which the model can interpret. Initially, each image is duplicated, with one copy downsampled by a specified scale factor. In our specific project, the scale factor is set as a hyperparameter of 4, resulting in an input image size of 128x128. Note that the convolution and pixel shuffle layers employed in our model do not impose constraints on the input image shape, allowing for the utilization of images of varying sizes, not just the square format we presently employ. Subsequently, the images are converted into tensors and incorporated into the PyTorch DataLoader object for seamless integration into the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b648436",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4265b0",
   "metadata": {},
   "source": [
    "![alt text](https://raw.githubusercontent.com/PSTAT197-F23/vignette-super-resolution/main/image/model_explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ an Efficient Sub-Pixel Convolutional Neural Network (ESPCN) as our chosen model. As illustrated in the diagram, the model is initially fed a low-resolution image, and a convolutional neural network (CNN) layer is applied to generate feature maps. The resulting number of channels surpasses the original three channels. With these channels, we applied the pixel shuffle method, a pivotal component of the sub-pixel convolution network, to transform these feature maps. The underlying concept assumes the existence of minute pixels between two physical pixels in the microscopic realm, motivating the use of a sub-pixel convolution network to unveil this relationship. As portrayed in the diagram, each pixel on the feature maps corresponds to a sub-pixel on the high-resolution image. The model encompasses three convolution layers, elevating the channel size to 48 ($3 \\times \\text{scale factor}^2$). Subsequently, the Spatial Pixel Shuffle (SPC) layer is employed with these channels to obtain the high-resolution image. The ReLU activation function is employed for its ability to introduce non-linearity to linear relationships without incurring heavy computational costs, facilitating longer training epochs and better results in less time. The Mean Squared Error (MSE) serves as our chosen loss function, with the original 512x512 image as the label and the down-sampled 128x128 picture as our input. The loss function is shown below:<br> \n",
    "$$\\ell(W_{1:L, b_{1:L}}) = \\frac{1}{r^2HW}\\sum\\limits_{x=1}^{rH}\\sum\\limits_{x=1}^{rW}\\left( I^{HR}_{x,y} - f^L_{x,y}(I^{LR}) \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb8f32",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b1303",
   "metadata": {},
   "source": [
    "The efficacy of our model is assessed through a comparative analysis of the input low-resolution images and the corresponding reconstructed high-resolution outputs, alongside the monitoring of the model's learning progress through the visualization of a loss graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5185f",
   "metadata": {},
   "source": [
    "![alt text](https://raw.githubusercontent.com/PSTAT197-F23/vignette-super-resolution/main/image/reference.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual representation presented above offers a comprehensive insight into the model's performance. The top row displays the original images used as input during the training phase, which we can see are downsized. In the second row, the output of our model is displayed, revealing a remarkable improvement in clarity and detail compared to the initial low-resolution images. This underscores the model's proficiency in recovering intricate details and sharpness that were not discernible in the original images. The bottom row displays the corresponding high-resolution outputs, each at a size of 512x512 pixels, facilitating a straightforward comparison. The discernible enhancement in image quality underscores the effectiveness of our model in reconstructing low-resolution images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e291c",
   "metadata": {},
   "source": [
    "![alt text](https://raw.githubusercontent.com/PSTAT197-F23/vignette-super-resolution/main/image/loss.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second image depicts training and testing loss curves, which are essential indicators of the model's learning process over time. The blue line graphically represents the training loss, a metric reflecting how adeptly the model fits the training data. A gradual decrease in the training loss over time is expected as the model learns from the training dataset. The training loss is tracked through the variable `loss_history`, which is systematically updated within the train function each time the loss is computed for a batch during training. The orange line denotes the test loss, assessing the model's generalization to new, unseen data. The test loss is recorded in the variable `test_loss_history`, which is updated every 5 iterations within the training loop when the test loss is calculated using the `test_dataloader`. A consistent downward trend in both loss values as training advances is indicative of positive learning dynamics, signifying the model's effective learning of the super-resolution task without exhibiting signs of overfitting at the present stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753d07e",
   "metadata": {},
   "source": [
    "These results are promising, indicating that our model holds the potential to substantially enhance the quality of low-resolution images. The observed trends in both training and test loss reinforce the notion that the model is learning effectively and progressing toward successful image reconstruction.\n",
    "\n",
    "However, there is a need for an additional interpretable metric, besides MSE, to precisely quantify the dissimilarity between the generated images and their original counterparts, and evaluate the model performance. \n",
    "\n",
    "Moreover, the absence of overfitting suggests that increasing the model size may potentially yield better results, which we haven't explore due to hardware constraints, prompting consideration for future investigations into model scalability and its impact on overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e09963",
   "metadata": {},
   "source": [
    "## Code Appendix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21aff7",
   "metadata": {},
   "source": [
    "### Run the scripts to reproduce the result. Do not run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b9333",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    return img\n",
    "\n",
    "class DatasetFromFolder(Dataset):\n",
    "    def __init__(self, image_dir, scale_factor):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n",
    "        self.tensor = transforms.ToTensor()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = load_img(self.image_filenames[index])\n",
    "        target = input.copy()\n",
    "    \n",
    "        input = self.tensor(input)\n",
    "        target = self.tensor(target)\n",
    "        \n",
    "        height, width = transforms.functional.get_image_size(input)\n",
    "        resize = transforms.Resize((int(height/self.scale_factor), int(width/self.scale_factor)), \n",
    "                                  transforms.InterpolationMode.BICUBIC, \n",
    "                                  antialias=True\n",
    "                                 )\n",
    "        input = resize(input)\n",
    "        del(resize)\n",
    "        \n",
    "        return input, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4f465",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5082fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 3 * scale_factor ** 2, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.clamp(x, 0.0, 1.0)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07c1bf",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d88c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# import dataset.py and model.py\n",
    "from dataset import *\n",
    "from model import *\n",
    "\n",
    "\n",
    "# main helper functions\n",
    "def swap(img):\n",
    "    img = img.swapaxes(0, 1)\n",
    "    img = img.swapaxes(1, 2)\n",
    "    return img\n",
    "\n",
    "def train(epoch, model):\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_history = []\n",
    "    epoch_test_loss_history = []\n",
    "    \n",
    "    for iteration, batch in enumerate(train_dataloader, 1):\n",
    "        img, target = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(img), target)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % 5 == 0:\n",
    "            tbatch = next(iter(test_dataloader))\n",
    "            timg, ttarget = tbatch[0].to(device), tbatch[1].to(device)\n",
    "            tloss = criterion(model(timg), ttarget)\n",
    "            \n",
    "            epoch_loss_history.append(loss.item())\n",
    "            epoch_test_loss_history.append(tloss.item())\n",
    "            \n",
    "            print(\"===> Epoch[{}]({}/{}): Loss: {:.6f}, Test Loss: {:.6f}\".format(\n",
    "                epoch+1, iteration, len(train_dataloader), loss.item(), tloss.item()))\n",
    "\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.6f}\".format(epoch+1, epoch_loss / len(train_dataloader)))\n",
    "    \n",
    "    return epoch_loss_history, epoch_test_loss_history\n",
    "\n",
    "\n",
    "# set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    \n",
    "# set hyperparameter\n",
    "scale_factor    = 4\n",
    "batch_size      = 32\n",
    "epoch           = 5\n",
    "learning_rate   = 0.0003\n",
    "criterion       = nn.MSELoss()\n",
    "\n",
    "\n",
    "# load data\n",
    "train_data = DatasetFromFolder(\"../data/train/wild\", scale_factor=scale_factor)\n",
    "test_data = DatasetFromFolder(\"../data/train/wild\", scale_factor=scale_factor)\n",
    "ref_data = DatasetFromFolder(\"../data/loss\", scale_factor=scale_factor)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "ref_dataloader = DataLoader(ref_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# create model\n",
    "model = Model(scale_factor=scale_factor).to(device)\n",
    "\n",
    "\n",
    "# train\n",
    "figure, ax = plt.subplots(3, epoch)\n",
    "figure.set_size_inches(20, 15)\n",
    "\n",
    "loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    epoch_loss_history, epoch_test_loss_history = train(i, model)\n",
    "    loss_history = loss_history + epoch_loss_history\n",
    "    test_loss_history = test_loss_history + epoch_test_loss_history\n",
    "    del(epoch_loss_history)\n",
    "    del(epoch_test_loss_history)\n",
    "    \n",
    "    ref = next(iter(ref_dataloader))[0]\n",
    "    ref_tgt = next(iter(ref_dataloader))[1]\n",
    "    ref_fit = model(ref.to(device)).cpu()\n",
    "    \n",
    "    ref = swap(ref.squeeze())\n",
    "    ref_tgt = swap(ref_tgt.squeeze())\n",
    "    ref_fit = swap(ref_fit.detach().numpy().squeeze())\n",
    "\n",
    "    ax[0, i].imshow(ref)\n",
    "    ax[1, i].imshow(ref_fit)\n",
    "    ax[2, i].imshow(ref_tgt)\n",
    "\n",
    "figure.savefig('../image/reference.jpg')\n",
    "\n",
    "\n",
    "# plot loss\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(loss_history)\n",
    "plt.plot(test_loss_history)\n",
    "plt.savefig('../image/loss.jpg')\n",
    "\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), '../model/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
